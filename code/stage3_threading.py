"""
Stage 3: Event Threading for Temporal Ordering
Supports both English-only and multilingual (English + Chinese) modes.
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Optional
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import json
import pickle
import re
from itertools import combinations

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, f1_score
import xgboost as xgb

from language_utils import (
    tokenize_text_for_tfidf,
    extract_words,
    get_bigrams_multilingual,
    extract_capitalized_words_multilingual,
    extract_numbers
)

try:
    import networkx as nx
    NETWORKX_AVAILABLE = True
except ImportError:
    NETWORKX_AVAILABLE = False


TEMPORAL_RELATIONS = {
    'SAME_DAY': 0,           # Same day coverage of same event
    'IMMEDIATE_UPDATE': 1,    # 1-2 days: Direct follow-up
    'SHORT_TERM_DEV': 2,      # 3-7 days: Story development
    'LONG_TERM_DEV': 3,       # 8-30 days: Ongoing story
    'DISTANT_RELATED': 4      # 30+ days: Same topic, distant events
}


def extract_numbers(text: str) -> set:
    """Extract all numbers from text."""
    return set(re.findall(r'\b\d+\b', str(text)))


def extract_capitalized_words(text: str) -> set:
    """Extract capitalized words."""
    if pd.isna(text):
        return set()
    words = re.findall(r'\b[A-Z][a-z]+\b', str(text))
    return set(w.lower() for w in words)


def get_bigrams(text: str) -> set:
    """Extract word bigrams."""
    words = text.lower().split()
    if len(words) < 2:
        return set()
    return set(zip(words[:-1], words[1:]))


def extract_threading_features_fixed(
    title_a: str,
    title_b: str,
    tfidf_vectorizer: TfidfVectorizer = None,
    language_mode: str = 'english'
) -> Dict[str, float]:
    """Extract threading features with language-aware processing."""
    features = {}

    text_a = str(title_a).lower().strip() if not pd.isna(title_a) else ""
    text_b = str(title_b).lower().strip() if not pd.isna(title_b) else ""
    orig_a = str(title_a) if not pd.isna(title_a) else ""
    orig_b = str(title_b) if not pd.isna(title_b) else ""

    # Language-aware word extraction
    if language_mode == 'multilingual':
        words_a = extract_words(text_a)
        words_b = extract_words(text_b)
    else:
        words_a = set(text_a.split())
        words_b = set(text_b.split())

    intersection = len(words_a & words_b)
    union = len(words_a | words_b)

    features['jaccard'] = intersection / union if union > 0 else 0
    features['common_words'] = intersection
    features['words_only_in_a'] = len(words_a - words_b)
    features['words_only_in_b'] = len(words_b - words_a)
    features['novelty_ratio'] = len(words_b - words_a) / len(words_b) if len(words_b) > 0 else 0

    features['len_a'] = len(words_a)
    features['len_b'] = len(words_b)
    features['len_diff'] = abs(len(words_a) - len(words_b))
    features['len_ratio'] = min(len(words_a), len(words_b)) / max(len(words_a), len(words_b)) if max(len(words_a), len(words_b)) > 0 else 0

    # Language-aware entity extraction
    if language_mode == 'multilingual':
        entities_a = extract_capitalized_words_multilingual(orig_a)
        entities_b = extract_capitalized_words_multilingual(orig_b)
    else:
        entities_a = extract_capitalized_words(orig_a)
        entities_b = extract_capitalized_words(orig_b)
    entity_intersection = len(entities_a & entities_b)
    entity_union = len(entities_a | entities_b)
    features['entity_overlap'] = entity_intersection / entity_union if entity_union > 0 else 0

    numbers_a = extract_numbers(orig_a)
    numbers_b = extract_numbers(orig_b)
    number_intersection = len(numbers_a & numbers_b)
    number_union = len(numbers_a | numbers_b)
    features['number_overlap'] = number_intersection / number_union if number_union > 0 else 0

    # Language-aware bigrams
    if language_mode == 'multilingual':
        bigrams_a = get_bigrams_multilingual(text_a)
        bigrams_b = get_bigrams_multilingual(text_b)
    else:
        bigrams_a = get_bigrams(text_a)
        bigrams_b = get_bigrams(text_b)
    bigram_intersection = len(bigrams_a & bigrams_b)
    bigram_union = len(bigrams_a | bigrams_b)
    features['bigram_overlap'] = bigram_intersection / bigram_union if bigram_union > 0 else 0

    # ==========================================================================
    # STATISTICALLY VALIDATED MARKERS
    # All markers passed chi-squared test (p < 0.05) with Cramér's V >= 0.05
    # Generated by analyze_linguistic_markers.py
    # ==========================================================================

    # Update indicators - English (validated)
    # - 'update': peak LONG_TERM_DEV, V=0.058
    # - 'again': peak DISTANT_RELATED, V=0.056
    # - 'more': peak SHORT_TERM_DEV, V=0.054
    # - 'now': peak LONG_TERM_DEV, V=0.053
    update_indicators_en = ['update', 'again', 'more', 'now']

    # Update indicators - Chinese (validated)
    # - '再': peak DISTANT_RELATED, V=0.100
    # - '仍': peak SHORT_TERM_DEV, V=0.065
    # - '更新': peak SHORT_TERM_DEV, V=0.059
    # - '又': peak DISTANT_RELATED, V=0.058
    # - '再次': peak DISTANT_RELATED, V=0.055
    update_indicators_zh = ['再', '仍', '更新', '又', '再次']

    update_indicators = update_indicators_en + (update_indicators_zh if language_mode == 'multilingual' else [])
    features['update_indicator_count'] = sum(1 for ind in update_indicators if ind in text_b)

    # Backward reference indicators - English (validated)
    # - 'after': peak IMMEDIATE_UPDATE, V=0.052
    backward_indicators_en = ['after']

    # Backward reference indicators - Chinese (validated)
    # - '因': peak IMMEDIATE_UPDATE, V=0.121 (strongest signal!)
    # - '后': peak DISTANT_RELATED, V=0.104
    # - '因为': peak IMMEDIATE_UPDATE, V=0.060
    # - '回应': peak IMMEDIATE_UPDATE, V=0.060
    # - '已被': peak SAME_DAY, V=0.054
    backward_indicators_zh = ['因', '后', '因为', '回应', '已被']

    backward_indicators = backward_indicators_en + (backward_indicators_zh if language_mode == 'multilingual' else [])
    features['backward_ref_count'] = sum(1 for ind in backward_indicators if ind in text_b)

    # Breaking news indicators - English (none passed validation threshold)
    # Note: English breaking news markers did not show significant discriminative power
    breaking_indicators_en = []

    # Breaking news indicators - Chinese (validated)
    # - '突发': peak SAME_DAY, V=0.081
    # - '爆': peak IMMEDIATE_UPDATE, V=0.065
    # - '独家': peak SAME_DAY, V=0.054
    breaking_indicators_zh = ['突发', '爆', '独家']

    breaking_indicators = breaking_indicators_en + (breaking_indicators_zh if language_mode == 'multilingual' else [])
    features['is_breaking_a'] = 1 if any(ind in text_a for ind in breaking_indicators) else 0
    features['is_breaking_b'] = 1 if any(ind in text_b for ind in breaking_indicators) else 0

    # Additional features based on validated markers
    # Statement verbs - Chinese only passed validation
    # - '说': peak IMMEDIATE_UPDATE, V=0.054
    if language_mode == 'multilingual':
        statement_verbs_zh = ['说']  # Only '说' passed validation
        features['statement_verb_b'] = 1 if any(v in text_b for v in statement_verbs_zh) else 0
    else:
        features['statement_verb_b'] = 0

    # Completion markers - Chinese (validated)
    # - '收官': peak DISTANT_RELATED, V=0.058
    # - '结束': peak IMMEDIATE_UPDATE, V=0.056
    # - '通过': peak LONG_TERM_DEV, V=0.052
    if language_mode == 'multilingual':
        completion_markers_zh = ['收官', '结束', '通过']
        features['completion_marker_b'] = 1 if any(m in text_b for m in completion_markers_zh) else 0
    else:
        features['completion_marker_b'] = 0

    # Process verbs - Chinese (validated)
    # - '检查': peak SAME_DAY, V=0.087
    # - '调查': peak IMMEDIATE_UPDATE, V=0.072
    if language_mode == 'multilingual':
        process_verbs_zh = ['检查', '调查']
        features['process_verb_count'] = sum(1 for v in process_verbs_zh if v in text_b)
    else:
        features['process_verb_count'] = 0

    # English-specific validated features
    # - 'claims': peak SAME_DAY, V=0.072
    # - 'probe': peak IMMEDIATE_UPDATE, V=0.072
    # - 'investigating': peak LONG_TERM_DEV, V=0.071
    # - 'ago': peak IMMEDIATE_UPDATE, V=0.060
    # - 'later': peak LONG_TERM_DEV, V=0.054
    claims_pattern = r'\bclaims\b'
    features['has_claims'] = 1 if re.search(claims_pattern, text_b, re.IGNORECASE) else 0

    process_verbs_en = ['probe', 'investigating']
    features['process_verb_en'] = sum(1 for v in process_verbs_en if re.search(r'\b' + v + r'\b', text_b, re.IGNORECASE))

    time_markers_en = ['ago', 'later']
    features['time_marker_en'] = sum(1 for v in time_markers_en if re.search(r'\b' + v + r'\b', text_b, re.IGNORECASE))

    # Time references - Chinese (validated)
    # - '日': peak DISTANT_RELATED, V=0.095
    # - '月': peak DISTANT_RELATED, V=0.080
    # - '今天': peak LONG_TERM_DEV, V=0.077
    if language_mode == 'multilingual':
        time_markers_zh = ['日', '月', '今天', '今晚', '明天']
        features['time_marker_zh'] = sum(1 for m in time_markers_zh if m in text_b)
    else:
        features['time_marker_zh'] = 0

    # Removal of has_denial - did not pass validation threshold

    if tfidf_vectorizer is not None:
        vec_a = tfidf_vectorizer.transform([text_a]).toarray()[0]
        vec_b = tfidf_vectorizer.transform([text_b]).toarray()[0]

        dot = np.dot(vec_a, vec_b)
        norm_a = np.linalg.norm(vec_a)
        norm_b = np.linalg.norm(vec_b)

        features['tfidf_cosine'] = dot / (norm_a * norm_b) if (norm_a * norm_b) > 0 else 0
        features['tfidf_euclidean'] = np.linalg.norm(vec_a - vec_b)
    else:
        features['tfidf_cosine'] = 0
        features['tfidf_euclidean'] = 0

    return features


# Feature names for reference
# All linguistic markers are statistically validated (p < 0.05, Cramér's V >= 0.05)
THREADING_FEATURE_NAMES = [
    # Lexical overlap
    'jaccard', 'common_words', 'words_only_in_a', 'words_only_in_b', 'novelty_ratio',
    # Length
    'len_a', 'len_b', 'len_diff', 'len_ratio',
    # Entity & number
    'entity_overlap', 'number_overlap', 'bigram_overlap',
    # Validated linguistic markers (shared)
    'update_indicator_count', 'backward_ref_count', 'is_breaking_a', 'is_breaking_b',
    # Validated Chinese-specific features
    'statement_verb_b', 'completion_marker_b', 'process_verb_count',
    # Validated English-specific features
    'has_claims', 'process_verb_en', 'time_marker_en',
    # Validated Chinese time markers
    'time_marker_zh',
    # TF-IDF
    'tfidf_cosine', 'tfidf_euclidean'
]


def extract_features_batch(
    df: pd.DataFrame,
    tfidf_vectorizer: TfidfVectorizer = None,
    language_mode: str = 'english'
) -> pd.DataFrame:
    """Extract features for batch with language-aware processing."""

    features_list = []

    for idx, row in df.iterrows():
        features = extract_threading_features_fixed(
            row['text_a'], row['text_b'],
            tfidf_vectorizer,
            language_mode
        )
        features_list.append(features)

        if (idx + 1) % 10000 == 0:
            print(f"  Processed {idx + 1}/{len(df)} pairs...")

    features_df = pd.DataFrame(features_list)

    return features_df


class EventThreadingClassifier:
    """Event threading classifier with multilingual support."""

    def __init__(self, model_type: str = 'xgboost', language_mode: str = 'english'):
        """Initialize classifier."""
        self.model_type = model_type
        self.language_mode = language_mode
        self.model = None
        self.tfidf_vectorizer = None
        self.label_encoder = LabelEncoder()
        self.feature_columns = None

    def fit(
        self,
        train_df: pd.DataFrame,
        val_df: pd.DataFrame = None
    ):
        """Train classifier."""
        print(f"Language mode: {self.language_mode}")

        all_texts = pd.concat([
            train_df['text_a'].apply(lambda x: str(x).lower() if not pd.isna(x) else ""),
            train_df['text_b'].apply(lambda x: str(x).lower() if not pd.isna(x) else "")
        ])

        # Language-aware TF-IDF configuration
        if self.language_mode == 'multilingual':
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=3000,
                ngram_range=(1, 2),
                tokenizer=tokenize_text_for_tfidf,
                token_pattern=None
            )
        else:
            self.tfidf_vectorizer = TfidfVectorizer(
                max_features=3000,
                ngram_range=(1, 2),
                stop_words='english'
            )
        self.tfidf_vectorizer.fit(all_texts)

        X_train = extract_features_batch(train_df, self.tfidf_vectorizer, self.language_mode)
        self.feature_columns = X_train.columns.tolist()
        y_train = self.label_encoder.fit_transform(train_df['temporal_relation'])

        print(f"Training: {len(X_train)} samples, {X_train.shape[1]} features")
        print(f"Classes: {self.label_encoder.classes_}")

        if self.model_type == 'xgboost':
            self.model = xgb.XGBClassifier(
                objective='multi:softmax',
                num_class=len(self.label_encoder.classes_),
                max_depth=6,
                learning_rate=0.1,
                n_estimators=150,
                subsample=0.8,
                colsample_bytree=0.8,
                random_state=42,
                n_jobs=-1,
                early_stopping_rounds=10 if val_df is not None else None
            )
        elif self.model_type == 'rf':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42,
                n_jobs=-1
            )
        else:
            self.model = GradientBoostingClassifier(
                n_estimators=100,
                max_depth=6,
                random_state=42
            )

        if val_df is not None and self.model_type == 'xgboost':
            X_val = extract_features_batch(val_df, self.tfidf_vectorizer, self.language_mode)
            y_val = self.label_encoder.transform(val_df['temporal_relation'])

            self.model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                verbose=False
            )
        else:
            self.model.fit(X_train, y_train)

        if hasattr(self.model, 'feature_importances_'):
            importance = self.model.feature_importances_
            sorted_idx = np.argsort(importance)[::-1]
            print("Top 5 features:")
            for rank, idx in enumerate(sorted_idx[:5], 1):
                name = self.feature_columns[idx]
                print(f"  {name}: {importance[idx]:.4f}")

    def predict(self, df: pd.DataFrame) -> np.ndarray:
        """Predict temporal relations."""
        X = extract_features_batch(df, self.tfidf_vectorizer, self.language_mode)
        X = X[self.feature_columns]

        y_pred = self.model.predict(X)
        labels = self.label_encoder.inverse_transform(y_pred)

        return labels

    def predict_proba(self, df: pd.DataFrame) -> np.ndarray:
        """Predict probabilities."""
        X = extract_features_batch(df, self.tfidf_vectorizer, self.language_mode)
        X = X[self.feature_columns]

        return self.model.predict_proba(X)

    def evaluate(self, test_df: pd.DataFrame) -> Dict[str, float]:
        """Evaluate classifier."""
        y_true = test_df['temporal_relation'].values
        y_pred = self.predict(test_df)

        accuracy = accuracy_score(y_true, y_pred)
        macro_f1 = f1_score(y_true, y_pred, average='macro')
        weighted_f1 = f1_score(y_true, y_pred, average='weighted')

        print(f"Accuracy: {accuracy:.4f}")
        print(f"Macro F1: {macro_f1:.4f}")
        print(f"Weighted F1: {weighted_f1:.4f}")

        print("\nClassification Report:")
        print(classification_report(y_true, y_pred))

        return {
            'accuracy': accuracy,
            'macro_f1': macro_f1,
            'weighted_f1': weighted_f1
        }

    def save(self, filepath: str):
        """Save model."""
        model_data = {
            'model': self.model,
            'tfidf_vectorizer': self.tfidf_vectorizer,
            'label_encoder': self.label_encoder,
            'feature_columns': self.feature_columns,
            'model_type': self.model_type,
            'language_mode': self.language_mode,
            'version': 'multilingual_v1'
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)

    @classmethod
    def load(cls, filepath: str) -> 'EventThreadingClassifier':
        """Load model."""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)

        language_mode = model_data.get('language_mode', 'english')
        classifier = cls(model_type=model_data['model_type'], language_mode=language_mode)
        classifier.model = model_data['model']
        classifier.tfidf_vectorizer = model_data['tfidf_vectorizer']
        classifier.label_encoder = model_data['label_encoder']
        classifier.feature_columns = model_data['feature_columns']

        return classifier


class TimelineBuilder:
    """Timeline builder."""
    
    def __init__(self, threading_classifier: EventThreadingClassifier = None):
        """Initialize timeline builder."""
        self.classifier = threading_classifier
    
    def build_timeline(
        self,
        articles: pd.DataFrame,
        method: str = 'hybrid'
    ) -> List[Dict]:
        """Build timeline."""
        if len(articles) == 0:
            return []
        
        if len(articles) == 1:
            return [self._article_to_dict(articles.iloc[0], position=0)]
        
        articles = articles.copy()
        articles['date'] = pd.to_datetime(articles['date'])
        
        if method == 'date':
            return self._build_by_date(articles)
        elif method == 'graph' and self.classifier is not None:
            return self._build_by_graph(articles)
        else:
            return self._build_hybrid(articles)
    
    def _article_to_dict(self, article: pd.Series, position: int, role: str = 'event') -> Dict:
        """Convert article to timeline entry."""
        date_val = article['date']
        if hasattr(date_val, 'date'):
            date_str = str(date_val.date())
        else:
            date_str = str(date_val)
        
        return {
            'position': position,
            'title': article['title'],
            'date': date_str,
            'role': role,
            'node_index': article.get('node_index', None)
        }
    
    def _build_by_date(self, articles: pd.DataFrame) -> List[Dict]:
        """Simple chronological ordering."""
        sorted_articles = articles.sort_values('date').reset_index(drop=True)
        
        timeline = []
        for i, (_, article) in enumerate(sorted_articles.iterrows()):
            role = 'initial' if i == 0 else 'update'
            timeline.append(self._article_to_dict(article, i, role))
        
        return timeline
    
    def _build_by_graph(self, articles: pd.DataFrame) -> List[Dict]:
        """Build using threading predictions."""
        if not NETWORKX_AVAILABLE:
            return self._build_by_date(articles)
        
        # Create pairs
        pairs = []
        indices = articles.index.tolist()
        
        for i, idx_a in enumerate(indices):
            for j, idx_b in enumerate(indices):
                if i < j:
                    art_a = articles.loc[idx_a]
                    art_b = articles.loc[idx_b]
                    pairs.append({
                        'text_a': art_a['title'],
                        'text_b': art_b['title'],
                        'idx_a': idx_a,
                        'idx_b': idx_b
                    })
        
        if len(pairs) == 0:
            return self._build_by_date(articles)
        
        pairs_df = pd.DataFrame(pairs)
        relations = self.classifier.predict(pairs_df)
        
        # Build graph
        G = nx.DiGraph()
        G.add_nodes_from(indices)
        
        for (_, row), relation in zip(pairs_df.iterrows(), relations):
            idx_a, idx_b = row['idx_a'], row['idx_b']
            date_a = articles.loc[idx_a, 'date']
            date_b = articles.loc[idx_b, 'date']
            
            if date_a <= date_b:
                earlier, later = idx_a, idx_b
            else:
                earlier, later = idx_b, idx_a
            
            if relation in ['IMMEDIATE_UPDATE', 'SHORT_TERM_DEV']:
                G.add_edge(earlier, later, relation=relation, weight=2)
            elif relation == 'SAME_DAY':
                G.add_edge(earlier, later, relation=relation, weight=1)
        
        try:
            order = list(nx.topological_sort(G))
        except nx.NetworkXUnfeasible:
            order = sorted(indices, key=lambda x: articles.loc[x, 'date'])
        
        timeline = []
        for i, idx in enumerate(order):
            article = articles.loc[idx]
            role = 'initial' if i == 0 else 'update'
            if G.in_degree(idx) > 1:
                role = 'development'
            timeline.append(self._article_to_dict(article, i, role))
        
        return timeline
    
    def _build_hybrid(self, articles: pd.DataFrame) -> List[Dict]:
        """Use dates as primary, content analysis for same-day ordering."""
        articles = articles.copy()
        articles['date_only'] = articles['date'].dt.date
        
        timeline = []
        position = 0
        
        for date, group in articles.groupby('date_only', sort=True):
            if len(group) == 1:
                article = group.iloc[0]
                role = 'initial' if position == 0 else 'update'
                timeline.append(self._article_to_dict(article, position, role))
                position += 1
            else:
                sub_timeline = self._order_same_day(group, position)
                timeline.extend(sub_timeline)
                position += len(sub_timeline)
        
        return timeline
    
    def _order_same_day(self, articles: pd.DataFrame, start_position: int) -> List[Dict]:
        """Order articles from the same day."""
        articles = articles.copy()
        
        def score_article(title):
            title_lower = str(title).lower()
            score = 0
            if any(ind in title_lower for ind in ['breaking', 'just in', 'alert']):
                score += 10
            score -= len(title_lower.split()) * 0.1
            return score
        
        articles['_score'] = articles['title'].apply(score_article)
        sorted_articles = articles.sort_values('_score', ascending=False)
        
        timeline = []
        for i, (_, article) in enumerate(sorted_articles.iterrows()):
            role = 'initial' if start_position + i == 0 else 'update'
            timeline.append(self._article_to_dict(article, start_position + i, role))
        
        return timeline


def kendall_tau(predicted_order: List, true_order: List) -> float:
    """Compute Kendall's Tau correlation."""
    from scipy.stats import kendalltau
    
    if len(predicted_order) != len(true_order):
        raise ValueError("Orders must have same length")
    
    if len(predicted_order) < 2:
        return 1.0
    
    pred_ranks = {item: i for i, item in enumerate(predicted_order)}
    true_ranks = {item: i for i, item in enumerate(true_order)}
    
    items = list(pred_ranks.keys())
    pred_array = [pred_ranks[item] for item in items]
    true_array = [true_ranks[item] for item in items]
    
    tau, _ = kendalltau(pred_array, true_array)
    
    return tau


def train_stage3(
    data_dir: str = '../output/prepared_data',
    model_output: str = '../output/models/stage3_threading.pkl',
    language_mode: str = 'english'
) -> Tuple[EventThreadingClassifier, Dict[str, float]]:
    """Train Stage 3 event threading classifier with multilingual support."""
    data_path = Path(data_dir)

    print(f"Language mode: {language_mode}")
    print("Loading data...")
    train_df = pd.read_csv(data_path / 'threading_train.csv')
    val_df = pd.read_csv(data_path / 'threading_val.csv')
    test_df = pd.read_csv(data_path / 'threading_test.csv')

    print(f"Train: {len(train_df)} pairs, Val: {len(val_df)} pairs, Test: {len(test_df)} pairs")

    classifier = EventThreadingClassifier(model_type='xgboost', language_mode=language_mode)
    print("Training classifier...")
    classifier.fit(train_df, val_df)
    print("Evaluating classifier...")
    metrics = classifier.evaluate(test_df)

    print("Saving model...")
    model_path = Path(model_output)
    model_path.parent.mkdir(parents=True, exist_ok=True)
    classifier.save(model_output)

    return classifier, metrics


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description='Train Stage 3 Event Threading Classifier')
    parser.add_argument('--data_dir', type=str, default='../output/prepared_data')
    parser.add_argument('--model_output', type=str, default='../output/models/stage3_threading.pkl')
    parser.add_argument('--language_mode', type=str, default='english',
                        choices=['english', 'multilingual'],
                        help='Language mode: english (original) or multilingual (EN+ZH)')

    args = parser.parse_args()

    classifier, metrics = train_stage3(
        data_dir=args.data_dir,
        model_output=args.model_output,
        language_mode=args.language_mode
    )